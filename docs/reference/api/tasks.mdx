---
title: "Tasks API"
description: "API reference for tasks, datasets, and evaluation management"
icon: "tasks"
---

## Task

Core class for defining agent evaluation scenarios.

```python
from hud import Task
```

### Constructor

```python
Task(
    prompt: str,
    mcp_config: dict[str, Any],
    id: str | None = None,
    setup_tool: MCPToolCall | list[MCPToolCall] | None = None,
    evaluate_tool: MCPToolCall | list[MCPToolCall] | None = None,
    metadata: dict[str, Any] = Field(default_factory=dict)
)
```

<ParamField body="prompt" type="str" required>
  The instruction given to the agent
</ParamField>

<ParamField body="mcp_config" type="dict" required>
  Environment connection configuration with variable substitution support
</ParamField>

<ParamField body="id" type="str" optional>
  Unique identifier for the task
</ParamField>

<ParamField body="setup_tool" type="MCPToolCall | list" optional>
  Tool(s) to initialize environment state
</ParamField>

<ParamField body="evaluate_tool" type="MCPToolCall | list" optional>
  Tool(s) to score agent performance
</ParamField>

<ParamField body="metadata" type="dict" optional>
  Additional task information for filtering and analysis
</ParamField>

### Environment Variable Substitution

Tasks support environment variable substitution in `mcp_config`:

```python
task = Task(
    prompt="Complete the task",
    mcp_config={
        "remote": {
            "url": "${MCP_URL:https://default.com}",
            "headers": {
                "Authorization": "Bearer ${API_KEY}"
            }
        }
    }
)
```

Format: `${VAR_NAME}` or `${VAR_NAME:default_value}`

## MCPToolCall

Defines a tool invocation.

```python
from hud import MCPToolCall
```

### Constructor

```python
MCPToolCall(
    name: str,
    arguments: dict[str, Any] | None = None
)
```

### Examples

```python
# Simple tool call
setup = MCPToolCall(name="reset")

# With arguments
setup = MCPToolCall(
    name="load_scenario",
    arguments={"scenario_id": "test_123"}
)

# Multiple tools
setup = [
    MCPToolCall(name="clear_data"),
    MCPToolCall(name="load_fixtures", arguments={"file": "test.json"})
]
```

## Dataset Functions

### load_tasks

Load tasks from HuggingFace or local source.

```python
from hud.datasets import load_tasks

tasks = load_tasks(
    dataset: str | Dataset,
    split: str = "train"
) -> list[Task]
```

<ParamField body="dataset" type="str | Dataset" required>
  HuggingFace dataset ID (e.g., "hud-evals/sheetbench-50") or Dataset object
</ParamField>

<ParamField body="split" type="str" default="train">
  Dataset split to load
</ParamField>

### save_tasks

Save tasks to HuggingFace.

```python
from hud.datasets import save_tasks

save_tasks(
    tasks: list[Task],
    repo_id: str,
    private: bool = True,
    revision: str | None = None
)
```

<ParamField body="tasks" type="list[Task]" required>
  List of Task objects to save
</ParamField>

<ParamField body="repo_id" type="str" required>
  HuggingFace repository ID (e.g., "my-org/my-tasks")
</ParamField>

<ParamField body="private" type="bool" default="true">
  Whether to make the dataset private
</ParamField>

<ParamField body="revision" type="str" optional>
  Git revision/branch name
</ParamField>

### run_dataset

Run agent on entire dataset with automatic parallelization.

```python
from hud.datasets import run_dataset
from hud.agents import ClaudeAgent

results = await run_dataset(
    name: str,
    dataset: str | Dataset | list[dict[str, Any]],
    agent_class: type[MCPAgent],
    agent_config: dict[str, Any] | None = None,
    max_concurrent: int = 50,
    metadata: dict[str, Any] | None = None,
    max_steps: int = 40,
    split: str = "train",
    auto_respond: bool = False
) -> list[Any]
```

<ParamField body="name" type="str" required>
  Name for the job/evaluation run
</ParamField>

<ParamField body="dataset" type="str | Dataset | list" required>
  HuggingFace dataset ID, Dataset object, or list of tasks
</ParamField>

<ParamField body="agent_class" type="type[MCPAgent]" required>
  Agent class to instantiate (e.g., ClaudeAgent)
</ParamField>

<ParamField body="agent_config" type="dict" optional>
  Configuration for agent (model, temperature, etc.)
</ParamField>

<ParamField body="max_concurrent" type="int" default="50">
  Maximum parallel task execution
</ParamField>

<ParamField body="metadata" type="dict" optional>
  Metadata for the job
</ParamField>

<ParamField body="max_steps" type="int" default="40">
  Maximum steps per task
</ParamField>

<ParamField body="split" type="str" default="train">
  Dataset split when loading from string
</ParamField>

## Job Tracking

### job

Context manager for tracking evaluation jobs.

```python
from hud.telemetry import job

with job(
    name: str,
    metadata: dict[str, Any] | None = None,
    job_id: str | None = None,
    dataset_link: str | None = None
) as j:
    # Run evaluations
    results = await run_dataset(...)
```

<ParamField body="name" type="str" required>
  Human-readable job name
</ParamField>

<ParamField body="metadata" type="dict" optional>
  Additional job metadata
</ParamField>

<ParamField body="job_id" type="str" optional>
  Explicit job ID (auto-generated if not provided)
</ParamField>

<ParamField body="dataset_link" type="str" optional>
  Link to dataset source
</ParamField>

## Trace

Result of agent execution.

```python
class Trace(BaseModel):
    reward: float              # 0.0 to 1.0
    done: bool                 # Whether agent completed
    content: str              # Result content/message
    isError: bool             # Whether error occurred
    trace_id: str | None      # Telemetry trace ID
    duration: float | None    # Execution time in seconds
    tool_calls: list[Any]     # List of tool invocations
    trace_url: str | None     # URL to view trace
```

## Usage Examples

### Basic Task Creation

```python
from hud import Task, MCPToolCall

# Simple task
task = Task(
    prompt="Navigate to the login page",
    mcp_config={"hudpython/hud-browser:latest": {}}
)

# With setup and evaluation
task = Task(
    prompt="Create a spreadsheet with sales data",
    mcp_config={"hudpython/hud-browser:latest": {}},
    setup_tool=MCPToolCall(name="goto_sheets"),
    evaluate_tool=MCPToolCall(
        name="check_spreadsheet",
        arguments={"has_data": True, "has_chart": True}
    ),
    metadata={
        "category": "spreadsheet",
        "difficulty": "medium"
    }
)
```

### Working with Datasets

```python
# Load existing benchmark
tasks = load_tasks("hud-evals/sheetbench-50")

# Filter tasks
medium_tasks = [
    t for t in tasks 
    if t.metadata.get("difficulty") == "medium"
]

# Run subset
results = await run_dataset(
    name="Medium Tasks Only",
    dataset=medium_tasks,  # Can pass list directly
    agent_class=ClaudeAgent,
    max_concurrent=10
)
```

### Creating Custom Datasets

```python
# Generate task variations
base_prompts = [
    "Click the submit button",
    "Fill out the contact form",
    "Navigate to settings"
]

tasks = []
for i, prompt in enumerate(base_prompts):
    for difficulty in ["easy", "medium", "hard"]:
        task = Task(
            id=f"task_{i}_{difficulty}",
            prompt=f"{prompt} (difficulty: {difficulty})",
            mcp_config={"browser-env": {}},
            metadata={
                "base_prompt": prompt,
                "difficulty": difficulty,
                "index": i
            }
        )
        tasks.append(task)

# Save to HuggingFace
save_tasks(tasks, "my-org/custom-benchmark")
```

### Batch Evaluation with Tracking

```python
from hud.telemetry import job
from hud.datasets import run_dataset

# Run with job tracking
with job("Nightly Benchmark Run", metadata={"version": "v2.1"}):
    results = await run_dataset(
        name="SheetBench Evaluation",
        dataset="hud-evals/sheetbench-50",
        agent_class=ClaudeAgent,
        agent_config={"model": "claude-3-5-sonnet-20241022"},
        max_concurrent=20
    )
    
    # Analyze results
    success_rate = sum(r.reward > 0.5 for r in results) / len(results)
    print(f"Success rate: {success_rate:.1%}")
```

### Custom Evaluation Logic

```python
# Override evaluation in task
task = Task(
    prompt="Complete the puzzle",
    mcp_config={"puzzle-env": {}},
    evaluate_tool=[
        # Multiple evaluation criteria
        MCPToolCall(name="check_complete"),
        MCPToolCall(name="check_time", arguments={"max_seconds": 120}),
        MCPToolCall(name="check_efficiency", arguments={"min_score": 0.8})
    ]
)

# Custom reward aggregation
async def evaluate_multi_criteria(agent, task):
    result = await agent.run(task)
    
    # If multiple evaluate tools, aggregate scores
    if isinstance(task.evaluate_tool, list):
        scores = []
        for tool in task.evaluate_tool:
            eval_result = await agent.call_tool(tool)
            scores.append(eval_result.reward)
        
        # Custom aggregation (e.g., weighted average)
        result.reward = sum(scores) / len(scores)
    
    return result
```

## Performance Tips

1. **Batch Size**: Adjust `max_concurrent` based on resource limits
2. **Task Caching**: Reuse loaded tasks across runs
3. **Metadata Filtering**: Use metadata for efficient task selection
4. **Environment Reuse**: Some environments support session persistence
5. **Result Storage**: Save results for later analysis

## Error Handling

```python
try:
    results = await run_dataset(
        dataset="my-tasks",
        agent_class=ClaudeAgent
    )
except Exception as e:
    print(f"Dataset run failed: {e}")
    
# Handle individual task failures
for i, result in enumerate(results):
    if result.isError:
        print(f"Task {i} failed: {result.content}")
```

## Next Steps

<CardGroup cols={2}>
<Card title="Create Tasks" icon="wrench" href="/evaluate-agents/custom-tasks">
  Build custom evaluation scenarios
</Card>

<Card title="Training Datasets" icon="database" href="/train-agents/datasets">
  Create datasets for RL training
</Card>
</CardGroup>


