# HUD Browser Environment

## Introduction

The `hud-browser` environment provides a remote Chromium browser instance, managed by Playwright, for agents to interact with websites. It's ideal for tasks involving web navigation, form filling, information retrieval, and testing web applications.

## Setup

Setup actions for the `hud-browser` are defined in the `setup` attribute of a [Task](../concepts/task.mdx) and executed by `hud.gym.make()`. They typically involve browser controller functions.

*   **`goto(url: str)`**: Navigates the browser to the specified `url`. Automatically prepends `http://` if no scheme is provided. Waits for `domcontentloaded` (up to 10s timeout) and adds a 1s wait for rendering.
    ```python
    # Example Task Setup:
    setup=[("goto", "https://google.com")]
    ```
*   **`load_html_content(html_string: str)`**: Loads the provided HTML string directly into the current browser page. This is particularly useful for creating tasks from datasets where the HTML state at a specific step is given, or for testing against static HTML content.
    ```python
    # Example Task Setup:
    html_content = "<html><body><h1>Hello, HUD!</h1></body></html>"
    setup=[("load_html_content", html_content)]
    ```
    Or, for a multi-line HTML string:
    ```python
    # Example Task Setup:
    html_content = \"\"\"
    <html>
        <body>
            <p>This is a test page.</p>
            <button id="myButton">Click Me</button>
        </body>
    </html>
    \"\"\"
    setup=[("load_html_content", html_content)]
    ```
*   **Other common setup functions coming soon:** `wait_for_element`, `click`, `type`, `set_cookies` etc. 

Refer to [Task Setup Configuration](../concepts/task.mdx#setup-configuration) for how to define these.

## Step Interaction

Agents interact with the browser environment by sending a list of [CLA Actions](../advanced/cla-details.mdx) to `env.step()`. An [Adapter](../concepts/adapter.mdx) typically handles the conversion from the agent model's output to the CLA format.

Common CLAs used with `hud-browser`:
*   [`ClickAction`](../advanced/cla-details.mdx#mouse-actions)
*   [`MoveAction`](../advanced/cla-details.mdx#mouse-actions)
*   [`TypeAction`](../advanced/cla-details.mdx#keyboard-actions)
*   [`PressAction`](../advanced/cla-details.mdx#keyboard-actions)
*   [`ScrollAction`](../advanced/cla-details.mdx#mouse-actions)
*   [`DragAction`](../advanced/cla-details.mdx#mouse-actions)
*   [`ResponseAction`](../advanced/cla-details.mdx#response-actions) (to submit a final text answer)

*See [CLA Action Details](../advanced/cla-details.mdx) for the full specification.*

## Evaluate

The `evaluate` attribute of a [Task](../concepts/task.mdx) defines how success is measured using `env.evaluate()`. This calls functions within the browser controller.

### Page State and Content Evaluators

These functions assess the overall state or content of the web page.

*   **`url_match(expected_url: str)`**: Checks if the current browser URL exactly matches `expected_url`. Returns `1.0` for a match, `0.0` otherwise.
    ```python
    # Example Task Evaluation:
    evaluate=("url_match", "https://google.com/search?q=expected")
    ```
*   **`page_contains(texts: list[str])`** (alias `contains_text`): Checks if *all* strings in `texts` are present in `page.content()`. Returns `1.0` if all texts are found, `0.0` otherwise.
    ```python
    # Example Task Evaluation:
    evaluate=("page_contains", ["Search Results", "About 1,000,000 results"])
    ```
*   **`sheet_contains(texts: list[str])`**: Custom function for Google Sheets. Returns `1.0` if any text is found, `0.0` otherwise.
    ```python
    # Example Task Evaluation:
    evaluate=("sheet_contains", ["Expected value in cell A1"])
    ```
*   **`cookie_exists(cookie_names: list[str])`**: Checks if all cookies in `cookie_names` exist in `context.cookies()`. Returns `1.0` if all exist, `0.0` otherwise.
    ```python
    # Example Task Evaluation:
    evaluate=("cookie_exists", ["session_id", "user_pref"])
    ```
*   **`cookie_match(name_value_pairs: list[str])`**: Checks if cookies exist *and* match expected values. `name_value_pairs` format: `[name1, value1, name2, value2, ...]`. Returns `1.0` if all match, `0.0` otherwise.
    ```python
    # Example Task Evaluation:
    evaluate=("cookie_match", ["user_id", "12345", "theme", "dark"])
    ```

### Agent Action Evaluators

These functions are designed to verify the agent's last performed action or the state of its interaction history. They are particularly useful for tasks derived from step-by-step datasets or when you need to confirm specific interactions.

*   **`selector_history(expected_selector: str, history_index: int = -1)`**: Checks if the element identified by `expected_selector` (CSS selector) was interacted with at the specified `history_index`. By default (`history_index = -1`), it checks the most recent interaction. Returns `1.0` if the selector matches the one recorded in the agent's action history at that index, `0.0` otherwise. This is typically used to verify a `CLICK` action on a specific element.
    ```python
    # Example: Verify the last clicked element had id="submitButton"
    evaluate=("selector_history", "#submitButton")
    
    # Example: Verify the first element interacted with had class="item-link"
    evaluate=("selector_history", ".item-link", 0)
    ```

*   **`verify_type_action(selector_and_value: list[str])`**: Specifically verifies that the last agent action was a `TYPE` action on a given element with the specified text.
    The `selector_and_value` argument must be a list containing two strings: `[css_selector, expected_text]`.
    Returns `1.0` if the last action matches these criteria, `0.0` otherwise.
    ```python
    # Example: Verify the agent typed "hello world" into input field with id="searchBox"
    evaluate=("verify_type_action", ["#searchBox", "hello world"])
    ```

*   **`history_length(expected_length: int)`**: Checks if the agent's recorded action history has a specific `expected_length`. 
    This is very useful for "negative" test cases where the agent is expected to *not* perform any action.
    Returns `1.0` if the length matches, `0.0` otherwise.
    ```python
    # Example: Verify the agent took exactly one action
    evaluate=("history_length", 1)

    # Example: Verify the agent took no actions (e.g., for "None of the above" scenarios)
    evaluate=("history_length", 0)
    ```

Refer to [Task Evaluation Configuration](../concepts/task.mdx#evaluation-configuration) for more details. 