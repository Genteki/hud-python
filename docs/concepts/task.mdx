---
title: 'Tasks and TaskSets'
description: 'Understanding Tasks and TaskSets for evaluation environments'
---

# Tasks and TaskSets

## Tasks

A Task represents an environment configuration that includes a problem statement and a reproducible setup for evaluating AI agents. Tasks are the foundational building blocks for creating evaluation environments.

### Task Overview

Tasks encapsulate everything needed to create a consistent evaluation environment, including:

- A problem statement that defines what the agent needs to accomplish
- A Dockerfile that ensures reproducible environment setup
- Configuration parameters for the specific evaluation scenario
- Any necessary resources or assets required by the environment

### Task Properties

| Property | Description |
| --- | --- |
| `id` | Unique identifier for the task |
| `envconfig` | Environment configuration containing: |
| | - `EnvType`: either a dockerfile, "browser", or "qa" (question-answer) |
| | - `EnvImpl`: either a URL to a zip file or the name of a Python package |
| `setup` | Setup instructions for the environment |
| `evaluator` | Evaluator specification for assessing agent responses |
| `problem_statement` | The text prompt provided to the agent |
| `config` | Additional configuration parameters specific to this task (dict[str, Any]) |

### Creating Custom Tasks

You can create custom tasks directly:

```python
import hud

# Create a custom task
task = hud.Task(
    id="custom_file_search",
    envconfig={
        "EnvType": "dockerfile",
        "EnvImpl": "FROM ubuntu:20.04\n..."  # Your custom Dockerfile
    },
    setup="Install necessary dependencies",
    evaluator="Match",
    problem_statement="Find the file named 'secret.txt' in the home directory",
    config={
        "time_limit": 300,
        "success_criteria": "file_found"
    }
)
```

## TaskSets

A TaskSet is a collection of related tasks designed for evaluating AI agents across multiple scenarios. TaskSets allow you to organize and access tasks in a structured way.

### TaskSet Overview

TaskSets serve as the primary way to access tasks within the HUD framework. They provide:

- A common way to group thematically related tasks
- Efficient loading of multiple tasks at once
- Integration with inspect-ai datasets for convenient task creation

### TaskSet Properties

| Property | Description |
| --- | --- |
| `id` | Unique identifier for the TaskSet |
| `description` | Description of what the TaskSet is designed to evaluate |
| `tasks` | List of Task objects contained in the TaskSet |

### Working with TaskSets

TaskSets are typically loaded using the `load` method:

```python
import hud
from hud import gym

# Load a TaskSet by ID
taskset = hud.TaskSet.load("osworld_tasks")

# Access tasks in the TaskSet
first_task = taskset.tasks[0]
print(f"First task: {first_task.problem_statement}")

# You can also access tasks using the index operator
second_task = taskset[1]
print(f"Second task: {second_task.problem_statement}")

# Get the number of tasks in the TaskSet
num_tasks = len(taskset)
print(f"Total tasks: {num_tasks}")

# Create an environment with a task from the TaskSet
env = gym.make("OSWorld-Ubuntu")
obs = await env.reset(task=first_task)
```

## Integration with inspect-ai

Both Tasks and TaskSets can be created from inspect-ai datasets:

```python
import hud
import inspect_ai

# Load a dataset from inspect-ai
dataset = inspect_ai.dataset.load("my_dataset")

# Create a TaskSet from the dataset
taskset = hud.TaskSet.from_inspect_dataset(dataset)

# Or, create a single task from a sample
sample = dataset.samples[0]
task = hud.Task.from_inspect_sample(sample)
```

## Best Practices

- Use standard TaskSets for benchmarking to ensure comparability across different agents
- When creating custom tasks, ensure the problem statement is clear and unambiguous
- Test custom tasks thoroughly to ensure they are properly configured
- Consider time limits appropriate for the complexity of the task
- When running evaluations, iterate through all tasks in a TaskSet to get a comprehensive assessment

## Related Concepts

- [Environment](/concepts/environment) - The runtime instance created from a Task
- [Trajectory](/concepts/trajectory) - Records of agent interactions in an environment
- [Job](/concepts/job) - Groups of related trajectories across multiple tasks