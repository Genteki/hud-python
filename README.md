<div align="left">
  <img src="https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/logo/hud_logo.svg" alt="HUD" width="150" style="margin-bottom: 20px;"/>
</div>

Evaluate and improve agent‚Äìenvironment systems. Wrap software as environments, define measurable evaluations, and train with RL ‚Äì locally or at scale.

### Are you a startup building agents?

[üìÖ Hop on a call](https://cal.com/jay-ram-z6st6w/demo) or [üìß founders@hud.so](mailto:founders@hud.so)

We're here to help with eval strategies, custom environments, or improving your agent via RL!

[![PyPI version](https://img.shields.io/pypi/v/hud-python)](https://pypi.org/project/hud-python/)

## What we help you do

- Environments: turn any app (browser, game, API) into a controllable environment with tools
- Evaluations: define setup/evaluate logic and run benchmarks (e.g., SheetBench)
- RL: train and evaluate via HUD‚Äôs Verifier-based gym (hud-vf-gym)

```text
Agent (LLM/Policy)  ‚îÄ‚îÄ tool calls (MCP JSON-RPC) ‚îÄ‚îÄ‚ñ∫  Environment (your software)
                                   ‚îÇ                        ‚ñ≤
                                   ‚ñº                        ‚îÇ
                           HUD Client/SDK           Tools you expose (setup/evaluate/act)
```

MCP (Model-Context Protocol) is the wire format we use to send tool calls. You rarely ‚Äúsee‚Äù MCP directly ‚Äì it‚Äôs just the protocol that lets any agent talk to any environment, locally or in Docker or in the cloud.

---

## 1) Environments

Wrap your software as an environment by exposing a small set of tools. Use **HudMCP** (a tiny wrapper around FastMCP) to register tool functions and run them in Docker so they‚Äôre reproducible and scalable. HudMCP adds handling for docker environments plus the handy `@mcp.initialize` / `@mcp.shutdown` decorators that wrap the ends of the MCP lifecycle.

Minimal call example (MCP client calling a local Docker image):

```python
import asyncio
import hud
from hud.client import MCPClient

async def main():
    with hud.trace("env-quickstart"):
        client = MCPClient(mcp_config={
            "local": {
                "command": "docker",
                "args": ["run","--rm","-i","hud-text-2048"]
            }
        })
        # Setup 2048 board, make a move, then evaluate
        await client.call_tool("setup", {"name": "board", "arguments": {"board_size": 4}})
        await client.call_tool("move", {"direction": "up"})
        result = await client.call_tool("evaluate", {"name": "max_number", "arguments": {"target": 64}})
        print(result)
        await client.close()

asyncio.run(main())
```

Start here:
- environments/README.md ‚Äì how to build an environment (with BaseHub + tools)
- examples/01_hello_2048.py ‚Äì end-to-end hello world using the 2048 environment
- examples/task_with_setup_eval.py ‚Äì minimal setup/evaluate without an LLM

---

## 2) Evaluations

Evaluations turn tasks into measurable outcomes. Your `evaluate` tool returns a reward (0‚Äì1), a done flag, and optional info.

Return shapes:

```python
from hud.tools.types import SetupResult, EvaluationResult

SetupResult(content="Seeded 5 items", info={"items": 5})
EvaluationResult(reward=0.8, done=False, content="Found 8/10 entries")
```

Recommended starting points:
- examples/sheet_bench.py ‚Äì SheetBench demo using a dataset ‚Üí TaskConfig ‚Üí agent run
- environments/remote_browser ‚Äì rich setup/evaluate hubs (navigate, cookies, page checks)

Run with Claude (quick demo):

```python
import asyncio
import hud
from datasets import load_dataset
from hud.datasets import run_dataset, TaskConfig
from hud.clients import MCPClient
from hud.agents import ClaudeAgent

async def main():
    # Run single task
    dataset = load_dataset("hud-evals/SheetBench-50", split="train")
    
    with hud.trace("sheetbench-claude-demo"):
        task = TaskConfig(**dataset[0])
        client = MCPClient(mcp_config=task.mcp_config)
        agent = ClaudeAgent(
            mcp_client=client,
            model="claude-sonnet-4-20250514",
            allowed_tools=["anthropic_computer"],
            initial_screenshot=True,
        )
        result = await agent.run(task, max_steps=40)
        print(f"Task: {task.prompt}")
        print(f"Result: {result.reward}")
        await client.close()

# Or run full dataset evaluation
async def run_full_dataset():
    results = await run_dataset(
        name="SheetBench-50 Evaluation",
        dataset="hud-evals/SheetBench-50", # HuggingFace dataset name
        agent_class=ClaudeAgent,
        agent_config={
            "model": "claude-sonnet-4-20250514",
            "allowed_tools": ["anthropic_computer"],
            "initial_screenshot": True,
        },
        max_concurrent=50,
        max_steps=150,
    )
    print(f"Average reward: {sum(r.reward for r in results if r) / len([r for r in results if r]):.2f}")

asyncio.run(main())
```

Tip: use `with hud.trace("run-name"):` to send traces to app.hud.so for debugging.

---

## 3) RL (hud-vf-gym + Verifiers)

HUD integrates with the Verifiers framework via `hud-vf-gym` (config-driven). A YAML config defines the agent-facing tools and maps them to MCP tools (action_mappings). You can run evaluations with the Verifiers CLI and train with GRPO.

Run evals (CLI):

```bash
vf-eval hud_vf_gym \
  --model gpt-4.1-mini \
  --env-args '{"taskset":"hud-evals/gmail-taskset","config_path":"./configs/default.yaml"}' \
  --num-examples 5
```

Train with GRPO (programmatic):

```python
from verifiers.trainers import GRPOTrainer, GRPOConfig
from hud_vf_gym import load_environment

env = load_environment(taskset="hud-evals/gmail-taskset", config_path="./configs/default.yaml")
cfg = GRPOConfig(model_name_or_path="your-model", num_train_epochs=3, per_device_train_batch_size=4)
trainer = GRPOTrainer(model=model, env=env, args=cfg, processing_class=tokenizer)
trainer.train()
```
Learn more:
- rl/README.md ‚Äì overview of RL frameworks (ART and Verifiers)
- rl/verifiers/README.md ‚Äì how hud-vf-gym + Verifiers work, configs and datasets
- rl/verifiers/configs/default.yaml (browser/computer), rl/verifiers/configs/2048.yaml (2048)

---

## Tooling & telemetry

### CLI Tools (`hud`)

**Installation:**
```bash
# Install globally with uv
uv tool install hud-python
```

**What's the difference?**

- **`hud debug <IMAGE>`** ‚Äì 5-phase checker (startup ‚Üí MCP handshake ‚Üí tools ‚Üí telemetry ‚Üí stress).
- **`hud analyze <IMAGE>`** ‚Äì Explore tools/resources (`--format json|markdown`). Requires debug phase 3.

```bash
hud debug hudpython/hud-remote-browser:latest
hud analyze hudpython/hud-remote-browser:latest
```

### Tracing & Debugging

- **Tracing**: `with hud.trace("run-name"):` - Stream any MCP-based trace to app.hud.so for debugging and visualization

---

## Next steps

- environments/ ‚Äì build an environment (BaseHub + tools)
- examples/ ‚Äì hello world, agent lifecycle, SheetBench
- rl/ ‚Äì RL training with ART and Verifiers frameworks

Install: `pip install hud-python`  ‚Ä¢  Issues/feedback welcome.


[MIT License](LICENSE)

```bibtex
@software{hud2025agentevalplatform,
  author = {HUD and Jay Ram and Lorenss Martinsons and Parth Patel and Oskars Putans and Govind Pimpale and Mayank Singamreddy and Nguyen Nhat Minh},
  title = {{HUD: An Evaluation Platform for Agents}},
  date = {2025-04},
  url = {https://github.com/hud-evals/hud-python},
  langid = {en}
}
```